{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T15:04:30.303147Z","iopub.status.busy":"2023-10-29T15:04:30.302288Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import re\n","import os\n","import string\n","import datetime as dt\n","import shutil\n","import json\n","import numpy as np\n","import pandas as pd\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset, Dataset\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing import text\n","\n","device = torch.device('cuda') if torch.cuda.is_available else 'cpu'\n","import datasets\n","\n","from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Cosine Simularity\n","class Similarity(nn.Module):\n","    \"\"\"\n","    Dot product or cosine similarity\n","    \"\"\"\n","\n","    def __init__(self, temp = 0.05):\n","        super().__init__()\n","        self.temp = temp\n","        self.cos = nn.CosineSimilarity(dim=-1)\n","\n","    def forward(self, x, y):\n","        return self.cos(x, y) / self.temp\n","\n","# Backbone Model\n","class Custom_tf_encoder(nn.Module):\n","    def __init__(self, used_word, embed_dim, nhead, encoder_layers, dropout=0.1, max_len=500):\n","        super().__init__()\n","        self.embedding_layer = nn.Embedding(used_word, embed_dim)\n","        self.encoders = nn.ModuleList([nn.TransformerEncoderLayer(\n","            d_model=embed_dim, \n","            nhead=nhead, \n","            dim_feedforward=(embed_dim*2), \n","            dropout=dropout, \n","            activation='gelu', \n","            batch_first=True) for _ in range(encoder_layers)])\n","#         self.avgpool = nn.AvgPool1d(kernel_size=embed_dim)\n","#         self.fc = nn.Linear(embed_dim, 2)\n","#         self.maxpool = nn.MaxPool2d(kernel_size = (max_len,1))\n","    \n","    def mk_padding_mask(self, text):\n","        # <pad>: 0\n","        return torch.eq(text, 0)\n","        \n","    def forward(self, text):\n","        x = self.embedding_layer(text)\n","        padding_mask = self.mk_padding_mask(text).to(x.device)\n","        for layer in self.encoders:\n","            x = layer(x, src_key_padding_mask=padding_mask)\n","#         x = self.maxpool(x) # (batch, 1, embed_dim)\n","#         x = x.squeeze(1) # (batch, embed_dim)\n","#         x = self.fc(x)\n","        \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# SIMCES Contrastive Model\n","class transformer_cl(nn.Module):\n","    def __init__(self, backbone):\n","        super().__init__()\n","        self.backbone = backbone\n","        self.Similarity = Similarity()\n","        \n","    def forward(self, x):\n","        emb_a = self.backbone(x)\n","        emb_b = self.backbone(x)\n","        emb_a, emb_b = emb_a[:, -1], emb_b[:, -1]\n","        cos_sim = self.Similarity(emb_a.unsqueeze(1), emb_b.unsqueeze(0))\n","        return cos_sim"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Pytorch Training\n","def cl_train(model, train_data, loss_fn, optimizer, device):\n","    cl_train_loss = 0\n","    model.train()\n","    for data in train_data:\n","        output = model(data.to(device))\n","        label = torch.arange(len(data)).to(device)\n","        \n","        optimizer.zero_grad()\n","        loss = loss_fn(output, label)\n","        loss.backward()\n","        optimizer.step()\n","        cl_train_loss += loss_fn(output, label)\n","    return cl_train_loss\n","\n","def cl_valid(model, valid_data, loss_fn, device):\n","    cl_valid_loss = 0\n","    model.eval()\n","    for data in valid_data:\n","        output = model(data.to(device))\n","        label = torch.arange(len(data)).to(device)\n","        cl_valid_loss += loss_fn(output, label)\n","        \n","    return cl_valid_loss\n","\n","\n","def Training(epochs, model, train_batch, valid_batch, loss_fn, optimizer, save_path, device):\n","    val_best_loss = float(\"inf\")\n","    for epoch in range(1, epochs+1):\n","        train_loss, valid_loss, best_valid_loss = 0, 0, float('inf')\n","        train_loss += cl_train(model, train_batch, loss_fn, optimizer, device)\n","        valid_loss += cl_valid(model, valid_batch, loss_fn, device)\n","        \n","        # best save only, early_stopping 구현\n","        if valid_loss < val_best_loss:\n","            es_patience = 0\n","            \n","            if not os.path.exists(os.path.dirname(save_path)):\n","                os.makedirs(os.path.dirname(save_path))\n","            torch.save(model, save_path)\n","        else:\n","            es_patience += 1\n","            if es_patience == 50:\n","                break\n","                print(f\"Train Stopped at Epoch {epoch}\")\n","        print(f\"Train_loss: {train_loss}, Valid_loss: {valid_loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Data Preprocessing\n","def custom_standardization(input_data):\n","    lowercase = tf.strings.lower(input_data)\n","    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n","    return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')\n","\n","def get_text_label(raw_text):\n","    prep_text = custom_standardization(raw_text).numpy()\n","    list_text = [data.decode('utf-8').replace('  ', ' ') for data in prep_text]\n","    return list_text\n","\n","def get_length_percentage(dataset, limit):\n","    count_under_limit = 0\n","    for x in dataset:\n","        if len(x) <= limit:\n","            count_under_limit += 1\n","    answer = (count_under_limit/len(dataset))*100\n","    print(f'{round(answer, 2)}% contains under {limit} words')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    unsupervised: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 50000\n","    })\n","})\n"]}],"source":["imdb = datasets.load_dataset(\"imdb\")\n","print(imdb)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
