{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import re\n","import os\n","import string\n","from sklearn.model_selection import train_test_split\n","import shutil\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-03T14:00:49.892181Z","iopub.status.busy":"2023-09-03T14:00:49.891727Z","iopub.status.idle":"2023-09-03T14:00:53.052804Z","shell.execute_reply":"2023-09-03T14:00:53.051583Z","shell.execute_reply.started":"2023-09-03T14:00:49.892149Z"},"trusted":true},"outputs":[],"source":["# 1. Bert는 pretrain과 Fine-tuningd이 따로 있다\n","# 2. pretrain에는 Next Sentence Prediction(NSP)와 Masked Language Model(MLM)으로 학습한다.\n","# 3. Bert pretrain에는 두 문장이 서로 관계가 있는지 Labeling이 된 데이터가 필요\n","# 4. 한국어 데이터셋 or 영어 데이터셋 찾아서 추후 업데이트 예정"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
